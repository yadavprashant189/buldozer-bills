{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting the price of a bulldozer using GridSearchCV","metadata":{}},{"cell_type":"markdown","source":"The dataset for this project can be found on this link: https://www.kaggle.com/c/bluebook-for-bulldozers/data\nIt has been divided into three parts:\n\nTrain.csv is the training set, which contains data through the end of 2011.\nValid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012.\nTest.csv is the test set, contains data from May 1, 2012 - November 2012.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the train and validation dataset\ndf = pd.read_csv('TrainAndValid.csv' ,\n                parse_dates=[\"saledate\"], \n                low_memory = False)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look for total number of empty spaces in the dataset\ndf.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the scatterplot of the date of sale of bulldozer vs. its sale price for first 2000 entries in each respective column\nfig,ax = plt.subplots()\nax.scatter(df[\"saledate\"][:2000],df[\"SalePrice\"][:2000]);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sorting the values of the dataset in ascending order with respet to date of sale\ndf.sort_values(by = [\"saledate\"],inplace=True, ascending=True)\ndf.head(30)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this step is done to copy the original dataset into a variable for fututre use\ndf_new = df.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adding a few extra parameters to the dataframe and getting rid of the date-month-year format\n#this step is important because the time data cannot be preprocessed in the abovedescribed format\ndf_new[\"saleYear\"] = df_new.saledate.dt.year\ndf_new[\"saleMonth\"] = df_new.saledate.dt.month\ndf_new[\"saleDay\"] = df_new.saledate.dt.day\ndf_new[\"saleDayOfYear\"] = df_new.saledate.dt.dayofyear\ndf_new.drop(\"saledate\",axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can see that the above dataset is ready to be preprocessed after arranging it in ascending order wrt. the sale year","metadata":{}},{"cell_type":"code","source":"for label,content in df_new.items():\n    \n    #in the new dataset , search for string data type cintents and convert them to category type\n    \n    if pd.api.types.is_string_dtype(content):\n        df_new[label] = content.astype(\"category\").cat.as_ordered()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new.to_csv('train_new.csv',\n              index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for label,value in df_new.items():\n    \n    #in the dataset if any numeric data type value is null , fill it with the median of the column\n    \n    if pd.api.types.is_numeric_dtype(value):\n        if pd.isnull(value).sum():\n            df_new[label] = content.fillna(value.median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking for leftover missing values\ndf_new.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating a list of those labels which have missing values\nmissing values\nmissing = []\nfor label in df_new:\n    if df_new[label].isna().sum():\n        missing.append(label)\n        \nmissing","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating a list of columns with non numeric data type content\nnot_numeric = []\nfor label,value in df_new.items():\n    if not pd.api.types.is_numeric_dtype(value):\n        not_numeric.append(label)\n        \nnot_numeric","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By default, the categorical code for a missing values is assigned -1 in Pandas. We can change this by adding 1 to the categorical code of missing values\n","metadata":{}},{"cell_type":"code","source":"for label,value in df_new.items():\n    if not pd.api.types.is_numeric_dtype(value):\n        \n        df_new[label] = pd.Categorical(value).codes+1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new.isna().sum().head(40)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have successfully cleaned our data, we can move on to model it using RandomForestRegressor with GridSearchCV to launch an exhaustive search for finding the best hyperparameters.","metadata":{}},{"cell_type":"markdown","source":"# Modelling and hyperparameter tuning\nHere we will divide the given dataset into training and validation set and use grid search CV to fing the best parameters ad find out the cost function using RMSLE. After that we will use the generated model to predict prices on the test set and find out the accuracy of the model.","metadata":{}},{"cell_type":"code","source":"x = df_new.drop(\"SalePrice\" , axis=1)\nx","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df_new[\"SalePrice\"]\ny","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting the dataset into the regression model\nnp.random.seed(42)\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_jobs = -1)\nmodel.fit(x , y);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.score(x,y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our random forest regression model has showcased 98.7 percent accuracy on the original set. Now we split it into training and validation data.","metadata":{}},{"cell_type":"code","source":"#validation set as described in the original problem from January 1, 2012 - April 30, 2012.\n\nvalid_set = df_new[df_new.saleYear==2012]\n\n#anything other than the data of theyear 2012 falls in the training set.\n\ntrain_set = df_new[df_new.saleYear!= 2012]\n\n#splitting the training and validation data into trainable set and target\n\nx_train , y_train = train_set.drop(\"SalePrice\",axis=1), train_set.SalePrice\nx_valid, y_valid = valid_set.drop(\"SalePrice\", axis=1), valid_set.SalePrice","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating rsmle function to return the root mean square log error\n\ndef rmsle(y_test,y_preds):\n    return np.sqrt(mean_squared_log_error(y_test,y_preds))\n\n# creating show_scores function to return the rmsle values\n\ndef show_scores(model):\n    train_preds = model.predict(x_train)\n    val_preds = model.predict(x_valid)\n    scores = {\"Valid RSMLE\": rmsle(y_valid, val_preds),\n             \"Training RSMLE\": rmsle(y_train,train_preds)}\n    return scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_scores(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can clearly see that our model is overfitting as it performs exceptinally on the training set when compared to validation set.","metadata":{}},{"cell_type":"markdown","source":"Grid Search CV will be used to find the best hyperparameters . However , because of its exhaustive nature, it may take hours to run even on a powerful machine. Hence,in order to save time and space, we will first reduce the parameters using RandomizedSearchCV and then apply GridSearchCV.","metadata":{}},{"cell_type":"code","source":"#making a grid of hyperparameters for RandomizedSearchCV\n\ngrid = {\"n_estimators\": np.arange(10,100,10),\n          \"max_depth\": [None,3,5,10],\n          \"min_samples_split\": np.arange(2,20,2),\n          \"min_samples_leaf\": np.arange(1,20,2),\n          \"max_features\": [0.5,1,\"sqrt\",\"auto\"],\n          \"max_samples\": [10000]}\n\n# creating a model of the RandomizedSearchCV by passing in the grid, the number of iterations(n_iter) and number of folds(cv)\n\ngs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,\n                                                   random_state=42),\n                             param_distributions=grid,\n                             n_iter = 10,\n                             cv=5,\n                             verbose=True)\ngs_model.fit(x_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extracting the best parameters based on random search\ngs_model.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_scores(gs_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can clearly see that our model is performing similarly on the training set and validation set.\nLet's use this information to create another grid that is to be passed in grid search CV.","metadata":{}},{"cell_type":"code","source":"grid_2 = {\"n_estimators\": [90],\n          \"max_depth\": [10,15],\n          \"min_samples_split\": [4,6,8],\n          \"min_samples_leaf\": [15],\n          \"max_features\": [\"sqrt\",\"auto\"],\n          \"max_samples\": [10000]}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preparing model for gridsearch CV by passing in the grid_2 and number of folds(cv).\n# number of itertions will not be passed as the grid search CV tries every single combination.\ngs_model_2 = GridSearchCV(RandomForestRegressor(n_jobs =-1,\n                                                random_state=42),\n                          param_grid = grid_2,\n                          cv=5,\n                          verbose=True)\n\ngs_model_2.fit(x_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs_model_2.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_scores(gs_model_2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As is clear from the above results, we were able to improve the performance of our model using grid search CV. We can further improve this by passing in more hyperparameters but that will require a significant amount of running time.","metadata":{}},{"cell_type":"markdown","source":"# Predictions on test data","metadata":{}},{"cell_type":"code","source":"test_set = pd.read_csv(\"Test.csv\",\n                     parse_dates=[\"saledate\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to preprocess the test data\ndef preprocess(df):\n    df[\"saleYear\"] = df.saledate.dt.year\n    df[\"saleMonth\"] = df.saledate.dt.month\n    df[\"saleDay\"] = df.saledate.dt.day\n    df[\"saleDayOfYear\"] = df.saledate.dt.dayofyear\n    df.drop(\"saledate\",axis=1, inplace=True)\n    for label,value in df.items():\n        if pd.api.types.is_numeric_dtype(value):\n            if pd.isnull(value).sum():\n                df[label] = value.fillna(value.median())\n        elif not pd.api.types.is_numeric_dtype(value):\n            df[label] = pd.Categorical(value).codes+1\n            \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = preprocess(test_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predict the price of bulldozers on test dataset\ntest_preds = gs_model_2.predict(test_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_preds = pd.DataFrame()\ndf_preds[\"SalesID\"] = test_set[\"SalesID\"]\ndf_preds[\"SalesPrice\"] = test_preds\ndf_preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_preds.to_csv(\"test_predition.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}